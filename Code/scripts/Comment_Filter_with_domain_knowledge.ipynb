{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import e inicialización"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "collapsed": true,
    "id": "1rNHLlQuFMyW",
    "outputId": "09f47263-1b4d-4242-9788-320f7c1ed0f0",
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.271958500Z",
     "start_time": "2025-02-22T05:49:51.217292300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchmetrics as tm\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Útiles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def load_glossary(path=\"../glossary/isoiecieee5652.csv\"):\n",
    "    glossary = pd.read_csv(path)\n",
    "    glossary['Term'] = glossary['Term'].str.strip()\n",
    "\n",
    "    return glossary\n",
    "\n",
    "\n",
    "def count_relevant_terms(comment, glossary):\n",
    "    count = 0\n",
    "\n",
    "    for term in glossary['Term']:\n",
    "        if term in comment:\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def build_count_feature_vector(size, comment, glossary, full=True):\n",
    "    feature_vector = torch.zeros(size)\n",
    "    relevant_count = count_relevant_terms(comment, glossary)\n",
    "\n",
    "    if full:\n",
    "        feature_vector[:] = relevant_count\n",
    "    else:\n",
    "        feature_vector[0] = relevant_count\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def build_position_feature_vector(size, comment, glossary, tokenizer):\n",
    "    feature_vector = torch.zeros(size)\n",
    "    tokenized_comment = tokenizer.encode(comment)\n",
    "\n",
    "    for term in glossary[\"Term\"]:\n",
    "        term_without_space = tokenizer.encode(term, add_special_tokens=False)\n",
    "        term_with_space = tokenizer.encode(\" \" + term, add_special_tokens=False)\n",
    "\n",
    "        apply_positional_match(feature_vector, tokenized_comment, term_without_space)\n",
    "        apply_positional_match(feature_vector, tokenized_comment, term_with_space)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def apply_positional_match(feature_vector, tokenized_comment, tokenized_term):\n",
    "    for i in range(len(tokenized_comment) - len(tokenized_term) + 1):\n",
    "\n",
    "        if tokenized_comment[i:i + len(tokenized_term)] == tokenized_term:\n",
    "            for j in range(len(tokenized_term)):\n",
    "                feature_vector[i + j] = 1\n",
    "\n",
    "\n",
    "def collate_function(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch])\n",
    "    feature_vectors = torch.tensor([item[\"feature_vector\"] for item in batch])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"label\": labels,\n",
    "        \"feature_vectors\": feature_vectors\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.352865300Z",
     "start_time": "2025-02-22T05:49:51.223088300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqhtDCi8E7DM"
   },
   "source": [
    "# Comment **Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FRW4dArbEx2q",
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.352865300Z",
     "start_time": "2025-02-22T05:49:51.239359300Z"
    }
   },
   "outputs": [],
   "source": [
    "class CommentDataset (Dataset):\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, max_token_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "        item = self.data.iloc[index]\n",
    "        comment = str(item.Review)\n",
    "        label = torch.FloatTensor(self.data.iloc[index, 1:])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "                                comment,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_token_len,\n",
    "                                return_token_type_ids=False,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                return_attention_mask=True,\n",
    "                                return_tensors='pt'\n",
    "        )\n",
    "        if len(encoding[\"input_ids\"].flatten()) != self.max_token_len:\n",
    "            print(\"Bad length, expected \", self.max_token_len, \", got \", encoding[\"input_ids\"].flatten().len)\n",
    "\n",
    "        return {'input_ids': encoding[\"input_ids\"].flatten(),\n",
    "                'attention_mask': encoding[\"attention_mask\"].flatten(),\n",
    "                'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "99w2If3vGt4g",
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.399742500Z",
     "start_time": "2025-02-22T05:49:51.254548200Z"
    }
   },
   "outputs": [],
   "source": [
    "class CommentDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_data, test_data, tokenizer, batch_size, max_token_len):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "        self.train_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = CommentDataset(self.train_data, self.tokenizer, self.max_token_len)\n",
    "        self.test_dataset = CommentDataset(self.test_data, self.tokenizer, self.max_token_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2,\n",
    "                          persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2,\n",
    "                          persistent_workers=True,)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2,\n",
    "                          persistent_workers=True,)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=2, shuffle=False,\n",
    "                          persistent_workers=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iTBPw9-8G1F2",
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.399742500Z",
     "start_time": "2025-02-22T05:49:51.263613400Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossCommentDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, k_fold, n_folds, split_seed, full_dataset, tokenizer, batch_size, max_token_len):\n",
    "        super().__init__()\n",
    "        self.full_dataset = full_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "        # actual fold number\n",
    "        self.k_fold = k_fold\n",
    "        # number of folds\n",
    "        self.n_folds = n_folds\n",
    "        # seed to control the randomness of fold splitting\n",
    "        self.split_seed = split_seed\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.split_seed)\n",
    "        all_splits = [k for k in kf.split(self.full_dataset)]\n",
    "        train_indexes, val_indexes = all_splits[self.k_fold]\n",
    "        train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n",
    "\n",
    "        self.train_dataset = CommentDataset(self.full_dataset.iloc[train_indexes], self.tokenizer, self.max_token_len)\n",
    "        self.test_dataset = CommentDataset(self.full_dataset.iloc[val_indexes], self.tokenizer, self.max_token_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2,\n",
    "                          persistent_workers=True,)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2,\n",
    "                          persistent_workers=True,)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2,\n",
    "                          persistent_workers=True,)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=2, shuffle=False,\n",
    "                          persistent_workers=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wsPbcptBG_bt",
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.399742500Z",
     "start_time": "2025-02-22T05:49:51.272473800Z"
    }
   },
   "outputs": [],
   "source": [
    "class CommentFilter(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.pretrained_model = AutoModel.from_pretrained(config['model'], return_dict=True).to(device)\n",
    "        self.linear_classifier = torch.nn.Linear(self.pretrained_model.config.hidden_size, 1).to(device)\n",
    "        self.sigmoid = torch.nn.Sigmoid().to(device)\n",
    "        torch.nn.init.xavier_uniform_(self.linear_classifier.weight)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.num_classes = 2\n",
    "\n",
    "        self.predictions = []\n",
    "        self.references = []\n",
    "\n",
    "        # metrics\n",
    "        self.accuracy = tm.Accuracy(task=\"binary\")\n",
    "        self.precision = tm.Precision(task=\"binary\")\n",
    "        self.recall = tm.Recall(task=\"binary\")\n",
    "        self.f1_score = tm.F1Score(task=\"binary\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # roberta layer\n",
    "        output = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # final logits\n",
    "        output = self.linear_classifier(output.last_hidden_state.mean(dim=1))\n",
    "        logits = self.sigmoid(output)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        loss, outputs = self.forward(input_ids, attention_mask, labels)\n",
    "\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "        metrics = {\n",
    "            \"val_loss\": loss,\n",
    "            \"accuracy\": self.accuracy(outputs, labels),\n",
    "            \"precision\": self.precision(outputs, labels),\n",
    "            \"recall\": self.recall(outputs, labels),\n",
    "            \"F1-score\": self.f1_score(outputs, labels)\n",
    "        }\n",
    "\n",
    "        self.predictions.append(outputs)\n",
    "        self.references.append(labels)\n",
    "\n",
    "        self.log_dict(metrics, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def test_step(self, batch, batch_index):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "\n",
    "        metrics = {\n",
    "            \"train_loss\": loss,\n",
    "            \"accuracy\": self.accuracy(outputs, labels),\n",
    "        }\n",
    "\n",
    "        self.predictions.append(outputs)\n",
    "        self.references.append(labels)\n",
    "\n",
    "        self.log_dict(metrics, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        return outputs\n",
    "\n",
    "    def on_test_start(self):\n",
    "        self.predictions.clear()\n",
    "        self.references.clear()\n",
    "\n",
    "    def on_test_end(self):\n",
    "        predictions = (torch.concat(self.predictions) > 0.5).int()\n",
    "        labels = torch.concat(self.references)\n",
    "        confusion_mat = ConfusionMatrix(task=\"binary\", num_classes=2).to(device)\n",
    "        print(\"Cunfusion Matrix: \\n\", confusion_mat(predictions, labels))\n",
    "\n",
    "    def get_metrics(self):\n",
    "\n",
    "        predictions = (torch.concat(self.predictions) > 0.5).int()\n",
    "        labels = torch.concat(self.references)\n",
    "        confusion_mat = ConfusionMatrix(task=\"binary\", num_classes=2).to(device)\n",
    "        conf_result = confusion_mat(predictions, labels)\n",
    "\n",
    "        tp = float(conf_result[0, 0].item())\n",
    "        fp = float(conf_result[0, 1].item())\n",
    "        fn = float(conf_result[1, 0].item())\n",
    "        tn = float(conf_result[1, 1].item())\n",
    "\n",
    "        print(confusion_mat, tp, fp, fn, tn)\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": (tp+tn)/(tp+fp+fn+tn),\n",
    "            \"precision\": tp/(tp+fp),\n",
    "            \"recall\": tp/(tp+fn),\n",
    "            \"F1-score\": 2*((tp/(tp+fp)*tp/(tp+fn))/(tp/(tp+fp)+tp/(tp+fn)))\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'])\n",
    "        total_steps = self.config['train_size']/self.config['batch_size']\n",
    "        warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1nPOVhwHPr-"
   },
   "source": [
    "# Comment Filter (Feature Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "93_F_vlKHbIl",
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.399742500Z",
     "start_time": "2025-02-22T05:49:51.298892400Z"
    }
   },
   "outputs": [],
   "source": [
    "class CommentDatasetFV (CommentDataset):\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, max_token_len, feature_vector_type=\"RELEVANT_COUNT\"):\n",
    "        super().__init__(data, tokenizer, max_token_len)\n",
    "        self.glossary = load_glossary()\n",
    "        self.feature_vector_type = feature_vector_type\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "        item = self.data.iloc[index]\n",
    "        comment = str(item.Review)\n",
    "        print(\"\\nComment \", index, \": \", comment)\n",
    "        label = torch.FloatTensor(self.data.iloc[index, 1:])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "                                comment,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_token_len,\n",
    "                                return_token_type_ids=False,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                return_attention_mask=True,\n",
    "                                return_tensors='pt'\n",
    "        )\n",
    "        feature_vector = None\n",
    "        if self.feature_vector_type == \"RELEVANT_COUNT\":\n",
    "            feature_vector = build_count_feature_vector(self.max_token_len, comment, self.glossary, full=True)\n",
    "\n",
    "        elif self.feature_vector_type == \"RELEVANT_POSITION\":\n",
    "            feature_vector = build_position_feature_vector(self.max_token_len, comment, self.glossary, self.tokenizer)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"ERROR: Invalid feature vector type: {self.feature_vector_type}\")\n",
    "\n",
    "        if len(encoding[\"input_ids\"].flatten()) != self.max_token_len:\n",
    "            print(\"Bad length, expected \", self.max_token_len, \", got \", len(encoding[\"input_ids\"].flatten().len()))\n",
    "\n",
    "        return {'input_ids': encoding[\"input_ids\"].flatten(),\n",
    "                'attention_mask': encoding[\"attention_mask\"].flatten(),\n",
    "                'label': label,\n",
    "                'feature_vector': feature_vector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Q2GLr7uDHgOt",
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.399742500Z",
     "start_time": "2025-02-22T05:49:51.298892400Z"
    }
   },
   "outputs": [],
   "source": [
    "class CommentDataModuleFV(CommentDataModule):\n",
    "\n",
    "    def __init__(self, train_data, test_data, tokenizer, batch_size, max_token_len):\n",
    "        super().__init__(train_data, test_data, tokenizer, batch_size, max_token_len)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = CommentDatasetFV(self.train_data, self.tokenizer, self.max_token_len)\n",
    "        self.test_dataset = CommentDatasetFV(self.test_data, self.tokenizer, self.max_token_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2,\n",
    "                          persistent_workers=True, collate_fn=collate_function)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2,\n",
    "                          persistent_workers=True, collate_fn=collate_function)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2,\n",
    "                          persistent_workers=True, collate_fn=collate_function)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2,\n",
    "                          persistent_workers=True, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MudVDM0bHk8-",
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.415373800Z",
     "start_time": "2025-02-22T05:49:51.307018600Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossCommentDataModuleFV(CrossCommentDataModule):\n",
    "\n",
    "    def __init__(self, k_fold, n_folds, split_seed, full_dataset, tokenizer, batch_size, max_token_len, feature_vector_type):\n",
    "        super().__init__(k_fold, n_folds, split_seed, full_dataset, tokenizer, batch_size, max_token_len)\n",
    "\n",
    "        self.feature_vector_type = feature_vector_type\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.split_seed)\n",
    "        all_splits = [k for k in kf.split(self.full_dataset)]\n",
    "        train_indexes, val_indexes = all_splits[self.k_fold]\n",
    "        train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n",
    "\n",
    "        self.train_dataset = CommentDatasetFV(self.full_dataset.iloc[train_indexes], self.tokenizer,\n",
    "                                              self.max_token_len, self.feature_vector_type)\n",
    "        self.test_dataset = CommentDatasetFV(self.full_dataset.iloc[val_indexes], self.tokenizer,\n",
    "                                             self.max_token_len, self.feature_vector_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WC1Hj0L9HopF",
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.415373800Z",
     "start_time": "2025-02-22T05:49:51.314045100Z"
    }
   },
   "outputs": [],
   "source": [
    "class CommentFilterFV(CommentFilter):\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.vector_size = self.pretrained_model.config.hidden_size + config['max_token_len']\n",
    "\n",
    "        self.linear_classifier = torch.nn.Linear(self.vector_size, 1).to(device)\n",
    "        torch.nn.init.xavier_uniform_(self.linear_classifier.weight)\n",
    "\n",
    "        self.use_mlp = config['use_mlp']\n",
    "\n",
    "        self.multilayer_perceptron = nn.Sequential(\n",
    "            nn.Linear(self.vector_size, self.vector_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.vector_size, self.vector_size),\n",
    "            nn.ReLU(),\n",
    "            self.linear_classifier\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, feature_vectors=None):\n",
    "\n",
    "        # roberta layer\n",
    "        output = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = output.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        output_with_domain_information = torch.cat((output, feature_vectors), dim=1)\n",
    "\n",
    "        if self.use_mlp:\n",
    "            logits = self.multilayer_perceptron(output_with_domain_information)\n",
    "        else:\n",
    "            logits = self.linear_classifier(output_with_domain_information)\n",
    "\n",
    "        # logits = self.classifier(output_with_domain_information)\n",
    "        logits = self.sigmoid(logits)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        feature_vectors = batch[\"feature_vector\"].to(device)\n",
    "        loss, outputs = self.forward(input_ids, attention_mask, labels, feature_vectors)\n",
    "\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        feature_vectors = batch[\"feature_vector\"].to(device)\n",
    "        loss, outputs = self(input_ids, attention_mask, labels, feature_vectors)\n",
    "\n",
    "        metrics = {\n",
    "            \"val_loss\": loss,\n",
    "            \"accuracy\": self.accuracy(outputs, labels),\n",
    "            \"precision\": self.precision(outputs, labels),\n",
    "            \"recall\": self.recall(outputs, labels),\n",
    "            \"F1-score\": self.f1_score(outputs, labels)\n",
    "        }\n",
    "\n",
    "        self.predictions.append(outputs)\n",
    "        self.references.append(labels)\n",
    "\n",
    "        self.log_dict(metrics, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def test_step(self, batch, batch_index):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        feature_vectors = batch[\"feature_vector\"].to(device)\n",
    "        loss, outputs = self(input_ids, attention_mask, labels, feature_vectors)\n",
    "\n",
    "        metrics = {\n",
    "            \"train_loss\": loss,\n",
    "            \"accuracy\": self.accuracy(outputs, labels),\n",
    "        }\n",
    "\n",
    "        self.predictions.append(outputs)\n",
    "        self.references.append(labels)\n",
    "\n",
    "        self.log_dict(metrics, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "szm-4aS0I-pl",
    "ExecuteTime": {
     "end_time": "2025-02-22T05:49:51.415373800Z",
     "start_time": "2025-02-22T05:49:51.327313400Z"
    }
   },
   "outputs": [],
   "source": [
    "def metrics_average(metrics_list):\n",
    "\n",
    "    metrics_sum = {\n",
    "        \"accuracy\": 0,\n",
    "        \"precision\": 0,\n",
    "        \"recall\": 0,\n",
    "        \"F1-score\": 0\n",
    "    }\n",
    "\n",
    "    for dic in metrics_list:\n",
    "        for metric in metrics_sum:\n",
    "            metrics_sum[metric] += dic[metric]\n",
    "\n",
    "    return {metric: sum_m / len(metrics_list) for metric, sum_m in metrics_sum.items()}\n",
    "\n",
    "def cross_validation_relevance(model_name, data_name, n_folds, config, mode=\"base\"):\n",
    "\n",
    "    model_path = \"../models/\" + model_name\n",
    "    data_path = \"../data/\" + data_name + \".csv\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    split_seed = 123\n",
    "    results = []\n",
    "\n",
    "    dataframe = pd.read_csv(data_path)\n",
    "\n",
    "    for k in range(n_folds):\n",
    "\n",
    "        data_module = None\n",
    "\n",
    "        # model init\n",
    "        if mode == \"base\":\n",
    "            data_module = CrossCommentDataModule(k, n_folds, split_seed, dataframe, tokenizer,\n",
    "                                                 batch_size=config['batch_size'], max_token_len=config['max_token_len'])\n",
    "            relevance_model = CommentFilter(config)\n",
    "\n",
    "        elif mode == \"FV\":\n",
    "            data_module = CrossCommentDataModuleFV(k, n_folds, split_seed, dataframe, tokenizer,\n",
    "                                                   batch_size=config['batch_size'], max_token_len=config['max_token_len'],\n",
    "                                                   feature_vector_type=config['FV_type'])\n",
    "            relevance_model = CommentFilterFV(config)\n",
    "        else:\n",
    "            raise ValueError(f\"ERROR: Invalid relevance mode: {mode}\")\n",
    "\n",
    "        data_module.setup()\n",
    "\n",
    "        config[\"train_size\"] = len(data_module.train_dataloader())\n",
    "\n",
    "        # training\n",
    "        trainer = pl.Trainer(max_epochs=config['n_epochs'], log_every_n_steps=5, enable_progress_bar=True)\n",
    "        trainer.fit(relevance_model, data_module)\n",
    "\n",
    "        # evaluation\n",
    "        trainer.test(relevance_model, data_module)\n",
    "        metrics = relevance_model.get_metrics()\n",
    "\n",
    "        results.append(metrics)\n",
    "        print(f\" {k+1}-Fold metrics: {metrics}\")\n",
    "\n",
    "        save_path = f\"../models/fine-tuned/relevance_model {model_name}(L+RC)-{data_name}-K{k + 1}.pth\"\n",
    "        torch.save(relevance_model, save_path)\n",
    "\n",
    "    print(\"Results :\", results)\n",
    "    print(\"Avg results: \", metrics_average(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORp-LCxWIqVv"
   },
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jAUkBJMzJEmF",
    "collapsed": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-02-22T05:49:51.337233500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                  | Type            | Params\n",
      "----------------------------------------------------------\n",
      "0 | pretrained_model      | RobertaModel    | 134 M \n",
      "1 | linear_classifier     | Linear          | 899   \n",
      "2 | sigmoid               | Sigmoid         | 0     \n",
      "3 | criterion             | BCELoss         | 0     \n",
      "4 | accuracy              | BinaryAccuracy  | 0     \n",
      "5 | precision             | BinaryPrecision | 0     \n",
      "6 | recall                | BinaryRecall    | 0     \n",
      "7 | f1_score              | BinaryF1Score   | 0     \n",
      "8 | multilayer_perceptron | Sequential      | 1.6 M \n",
      "----------------------------------------------------------\n",
      "136 M     Trainable params\n",
      "0         Non-trainable params\n",
      "136 M     Total params\n",
      "546.062   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30efa91fef3d4cabb86caab811a3b154"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "dataset = \"templerun2_labeled\"\n",
    "model = \"BERTweet - base\"\n",
    "\n",
    "# fine_tuned = \"../models/fine-tuned/comment_relevance_detector (facebook).pth\"\n",
    "n_folds = 5\n",
    "\n",
    "config = {\n",
    "    'model': \"../models/\" + model,\n",
    "    'batch_size': 16,\n",
    "    'lr': 2e-5,\n",
    "    'warmup': 0.2,\n",
    "    'train_size': None,\n",
    "    'weight_decay': 0.001,\n",
    "    'max_token_len': 130,\n",
    "    'n_epochs': 2,\n",
    "    'FV_type': 'RELEVANT_COUNT',    # RELEVANT_COUNT or RELEVANT_POSITION\n",
    "    'use_mlp': False,\n",
    "    'mlp_dimension': 100\n",
    "}\n",
    "\n",
    "cross_validation_relevance(model, dataset, n_folds, config, \"FV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WqhtDCi8E7DM",
    "T1nPOVhwHPr-",
    "ZLyBEPNMHuw-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
